{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPg7lENDIF+TPRhXqK37VHC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Ejercicio ##\n","1) Levantar la base finalOrders.txt creada en el ejercicio anterior.  \n","2) Analizar la cantidad de registros y columnas que tiene la base. Utilizar la función shape para dicho punto. [Documentación](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.shape.html)  \n","3) Realizar una división de la base de un 80% para entrenar y un 20% para validar el modelo. ¿Cuántos registros tenemos para entrenar? ¿Y para validar?  Recordar de usar el parámetro random_state para que siempre las particiones sean las mismas. Al parametro usar el valor 30 en random_state.  \n","4) Crear un XGBoost con los parámetros por defecto. ¿Cuál es el AUC del modelo?  \n","5) ¿Qué variable es la que posee mayor explicación según _Feature Importance_?\n","6) Modificar los hiperparámetros del XGBoost hasta conseguir un modelo que de resultados mejores al creado en el punto 4.  \n","7) Crear un modelo tipo LightGBM, utilizar los siguientes hiperparámetros:\n","*   máxima profundidad = 3\n","*   estimadores = 500\n","*   learning rate = 0.01\n","*   Resto de los parámetros por defecto. \n","\n","8) ¿Qué AUC obtuvimos?  \n","9) Graficar las curvas AUC del XGBoost y del LightGBM juntas. ¿Qué modelo es mejor?  \n","10) ¿Cuál es la principal diferencia entre un Gradient Boosting y el XGBoost?.  \n","11) Crear un Gradient Boosting con el que se consiga mayor AUC que el XGBoost con los parámetros de defecto."],"metadata":{"id":"37pw9trZY-5h"}},{"cell_type":"code","source":[],"metadata":{"id":"2VqeLwQJWlkA"},"execution_count":null,"outputs":[]}]}